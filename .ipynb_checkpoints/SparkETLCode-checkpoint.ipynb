{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "00ef1b6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "table{float:left;border-style:solid;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "table{float:left;border-style:solid;}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "272b8f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('etl').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2dbafd57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://Sharad:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>etl</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x17c3cee4d48>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22684813",
   "metadata": {},
   "source": [
    "##### Types are imported for creating input schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c11a1b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdc7d8c",
   "metadata": {},
   "source": [
    "##### The input schema is created according to the data in the csv file imported using scoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72743920",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType(\n",
    "    [\n",
    "        StructField(\"year\", IntegerType(), True),\n",
    "        StructField(\"month\", StringType(), True),\n",
    "        StructField(\"day\", IntegerType(), True),\n",
    "        StructField(\"weekday\", StringType(), True),\n",
    "        StructField(\"hour\", IntegerType(), True),\n",
    "        StructField(\"atm_status\", StringType(), True),\n",
    "        StructField(\"atm_id\", StringType(), True),\n",
    "        StructField(\"atm_manufacturer\", StringType(), True),\n",
    "        StructField(\"atm_location\", StringType(), True),\n",
    "        StructField(\"atm_streetname\", StringType(), True),\n",
    "        StructField(\"atm_street_number\", IntegerType(), True),\n",
    "        StructField(\"atm_zipcode\", IntegerType(), True),\n",
    "        StructField(\"atm_lat\", DoubleType(), True),\n",
    "        StructField(\"atm_lon\", DoubleType(), True),\n",
    "        StructField(\"currency\", StringType(), True),\n",
    "        StructField(\"card_type\", StringType(), True),\n",
    "        StructField(\"service\", StringType(), True),\n",
    "        StructField(\"message_code\", StringType(), True),\n",
    "        StructField(\"message_text\", StringType(), True),\n",
    "        StructField(\"weather_lat\", DoubleType(), True),\n",
    "        StructField(\"weather_lon\", DoubleType(), True),\n",
    "        StructField(\"weather_city_id\", IntegerType(), True),\n",
    "        StructField(\"weather_city_name\", StringType(), True),\n",
    "        StructField(\"temp\", IntegerType(), True),\n",
    "        StructField(\"preasure\", IntegerType(), True),\n",
    "        StructField(\"humidity\", IntegerType(), True),\n",
    "        StructField(\"wind_speed\", IntegerType(), True),\n",
    "        StructField(\"wind_deg\", IntegerType(), True),\n",
    "        StructField(\"rain_3h\", DoubleType(), True),\n",
    "        StructField(\"clouds_all\", IntegerType(), True),\n",
    "        StructField(\"weather_id\", IntegerType(), True),\n",
    "        StructField(\"weather_main\", StringType(), True),\n",
    "        StructField(\"weather_description\", StringType(), True),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9dbc1b",
   "metadata": {},
   "source": [
    "##### Importing Data from HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9711f9b",
   "metadata": {},
   "source": [
    "1. The path of hdfs where the imported data is stored\n",
    "2. The data is stored in csv format where each field is enclosed without quotes\n",
    "\n",
    "\n",
    "<font color='red'>\\* The numbered list are comments for the code bellow</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c03def8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path='./csv/imported_data.csv'\n",
    "temp_atm = spark.read.load(path,format='csv',schema=schema,quote='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a64cbd",
   "metadata": {},
   "source": [
    "Here I have used rand function to generate random values for the column transaction amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3112be81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- weekday: string (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- atm_status: string (nullable = true)\n",
      " |-- atm_id: string (nullable = true)\n",
      " |-- atm_manufacturer: string (nullable = true)\n",
      " |-- atm_location: string (nullable = true)\n",
      " |-- atm_streetname: string (nullable = true)\n",
      " |-- atm_street_number: integer (nullable = true)\n",
      " |-- atm_zipcode: integer (nullable = true)\n",
      " |-- atm_lat: double (nullable = true)\n",
      " |-- atm_lon: double (nullable = true)\n",
      " |-- currency: string (nullable = true)\n",
      " |-- card_type: string (nullable = true)\n",
      " |-- service: string (nullable = true)\n",
      " |-- message_code: string (nullable = true)\n",
      " |-- message_text: string (nullable = true)\n",
      " |-- weather_lat: double (nullable = true)\n",
      " |-- weather_lon: double (nullable = true)\n",
      " |-- weather_city_id: integer (nullable = true)\n",
      " |-- weather_city_name: string (nullable = true)\n",
      " |-- temp: integer (nullable = true)\n",
      " |-- preasure: integer (nullable = true)\n",
      " |-- humidity: integer (nullable = true)\n",
      " |-- wind_speed: integer (nullable = true)\n",
      " |-- wind_deg: integer (nullable = true)\n",
      " |-- rain_3h: double (nullable = true)\n",
      " |-- clouds_all: integer (nullable = true)\n",
      " |-- weather_id: integer (nullable = true)\n",
      " |-- weather_main: string (nullable = true)\n",
      " |-- weather_description: string (nullable = true)\n",
      " |-- transaction_amount: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import rand\n",
    "temp_atm = temp_atm.withColumn('transaction_amount',(rand()*10000).cast('int'))\n",
    "temp_atm.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ecafd5",
   "metadata": {},
   "source": [
    "##### The count of the imported table using scoop was <u>2468572</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d07ce148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The count of records in the imported file is 2468572\n"
     ]
    }
   ],
   "source": [
    "print('The count of records in the imported file is',temp_atm.count())\n",
    "# atm.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c419b42a",
   "metadata": {},
   "source": [
    "- ## Fact Table\n",
    "    1. [FACT_ATM_TRANS](#FACT_ATM_TRANS)\n",
    "- ## Dimention Table\n",
    "    1. [DIM_DATE](#DIM_DATE)\n",
    "    2. [DIM_LOCATION](#DIM_LOCATION)\n",
    "    3. [DIM_ATM](#DIM_ATM)\n",
    "    4. [DIM_CARD_TYPE](#DIM_CARD_TYPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cf229c",
   "metadata": {},
   "source": [
    "##### The following imports will be used for creating ID for the fact and dimenssion tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f26a878",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import row_number,lit\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "517ba944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- weekday: string (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- atm_status: string (nullable = true)\n",
      " |-- atm_id: string (nullable = true)\n",
      " |-- atm_manufacturer: string (nullable = true)\n",
      " |-- atm_location: string (nullable = true)\n",
      " |-- atm_streetname: string (nullable = true)\n",
      " |-- atm_street_number: integer (nullable = true)\n",
      " |-- atm_zipcode: integer (nullable = true)\n",
      " |-- atm_lat: double (nullable = true)\n",
      " |-- atm_lon: double (nullable = true)\n",
      " |-- currency: string (nullable = true)\n",
      " |-- card_type: string (nullable = true)\n",
      " |-- service: string (nullable = true)\n",
      " |-- message_code: string (nullable = true)\n",
      " |-- message_text: string (nullable = true)\n",
      " |-- weather_lat: double (nullable = true)\n",
      " |-- weather_lon: double (nullable = true)\n",
      " |-- weather_city_id: integer (nullable = true)\n",
      " |-- weather_city_name: string (nullable = true)\n",
      " |-- temp: integer (nullable = true)\n",
      " |-- preasure: integer (nullable = true)\n",
      " |-- humidity: integer (nullable = true)\n",
      " |-- wind_speed: integer (nullable = true)\n",
      " |-- wind_deg: integer (nullable = true)\n",
      " |-- rain_3h: double (nullable = true)\n",
      " |-- clouds_all: integer (nullable = true)\n",
      " |-- weather_id: integer (nullable = true)\n",
      " |-- weather_main: string (nullable = true)\n",
      " |-- weather_description: string (nullable = true)\n",
      " |-- transaction_amount: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temp_atm.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66a6c6f",
   "metadata": {},
   "source": [
    "## Creating Location Dimension table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea63ecc4",
   "metadata": {},
   "source": [
    "### **DIM_LOCATION**\n",
    "|Columns|Data Type|Constriant|\n",
    "| :- | :- |:-|\n",
    "|location_id|int|**Primary**|\n",
    "|location|string|\n",
    "|street_name|string|\n",
    "|street_number|int|\n",
    "|zipcode|int|\n",
    "|lat|double|\n",
    "|lon|double|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77be69d6",
   "metadata": {},
   "source": [
    "2. A new column is created called 'location_id'.\n",
    "3. 'location_id' is to be filled with value zero.\n",
    "4. The data frame is rearranged and the headers are renamed.\n",
    "\n",
    "\n",
    "12. Distinct will remove the duplicate.\n",
    "13. The column *card_type_id* is selected for updating its values\n",
    "14. IDs are generated such that the ID starts with number 2\n",
    "\n",
    "<font color='red'>\\* The numbered list are comments for the code bellow</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b74176c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_loc = temp_atm.withColumn(\n",
    "    'location_id',\n",
    "    lit(0).cast('int')\n",
    ").select(\n",
    "    'location_id',\n",
    "    temp_atm.atm_location.alias('location'),\n",
    "    temp_atm.atm_streetname.alias('street_name'),\n",
    "    temp_atm.atm_street_number.alias('street_number'),\n",
    "    temp_atm.atm_zipcode.alias('zipcode'),\n",
    "    temp_atm.atm_lat.alias('lat'),\n",
    "    temp_atm.atm_lon.alias('lon')\n",
    ").distinct().withColumn(\n",
    "    'location_id',\n",
    "    row_number().over(Window().orderBy('location','street_name'))*2+lit(20001)\n",
    ")\n",
    "\n",
    "# print(dim_loc.count())\n",
    "# dim_loc.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2de8ce",
   "metadata": {},
   "source": [
    "## Creating Card Type Dimension table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6939842",
   "metadata": {},
   "source": [
    "### **DIM_CARD_TYPE**\n",
    "|Columns|Data Type|Constriant|\n",
    "|:-|:-|:-|\n",
    "|card_type_id|int|**Primary**|\n",
    "|card_type|string|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefd15ab",
   "metadata": {},
   "source": [
    "1. The distinct values are selected from *card_type*.\n",
    "2. A new column *card_type_id* is created for storing IDs\n",
    "3. IDs are generated such that the ID starts with number 3\n",
    "\n",
    "\n",
    "4. The columns are rearranged.\n",
    "\n",
    "<font color='red'>\\* The numbered list are comments for the code bellow</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74bedde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_card_type = temp_atm.select('card_type').distinct().withColumn(\n",
    "    'card_type_id',\n",
    "    row_number().over(Window().orderBy('card_type'))*lit(2)+lit(30001)\n",
    ").select('card_type_id','card_type')\n",
    "# dim_card_type.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459600ab",
   "metadata": {},
   "source": [
    "## Creating ATM Dimension table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89731b90",
   "metadata": {},
   "source": [
    "### **DIM_ATM**\n",
    "|Columns|Data Type|Constriant|\n",
    "|:-|:-|:-|\n",
    "|atm_id|int|**Primary**|\n",
    "|atm_number|string|\n",
    "|atm_manufacturer|string|\n",
    "|atm_location_id|int|**Foreign(dim_loc.location_id)**|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866d628f",
   "metadata": {},
   "source": [
    "The *col* fulction will bel used for altering the headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "96b440d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b094ee",
   "metadata": {},
   "source": [
    "1. Creating a dataframe *dim_atm_s1* for storing the partial data collected from *temp_atm* dataframe. The columns are selected for performing join operation with *dim_loc* and the column *atm_id* is renamed as *atm_number*\n",
    "\n",
    "\n",
    "10. The the duplicates are removed using distinct.\n",
    "11. A new column called *atm_id* is created to storing ID.\n",
    "12. The ID values are generated such that the ID starts with number 4\n",
    "13. The Columns are rearranged.\n",
    "\n",
    "<font color='red'>\\* The numbered list are comments for the code bellow</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "19771b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_atm_s1 = temp_atm.select(\n",
    "    col('atm_id').alias('atm_number'),\n",
    "    'atm_manufacturer',\n",
    "    'atm_location',\n",
    "    'atm_streetname',\n",
    "    'atm_street_number',\n",
    "    'atm_zipcode',\n",
    "    'atm_lat',\n",
    "    'atm_lon'\n",
    ").distinct().withColumn(\n",
    "    'atm_id',\n",
    "    row_number().over(Window().orderBy('atm_number'))*lit(2)+lit(40001)\n",
    ").select(\n",
    "    'atm_id',\n",
    "    'atm_number',\n",
    "    'atm_manufacturer',\n",
    "    'atm_location',\n",
    "    'atm_streetname',\n",
    "    'atm_street_number',\n",
    "    'atm_zipcode',\n",
    "    'atm_lat',\n",
    "    'atm_lon'\n",
    ")\n",
    "# dim_atm_s1.count()\n",
    "# dim_atm.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5bccc9",
   "metadata": {},
   "source": [
    "1. The two dataframes *dim_atm_s1* and *dim_loc* are joined with their respective columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "66439d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_atm_s2 = dim_atm_s1.join(\n",
    "    dim_loc,\n",
    "    (dim_atm_s1['atm_location']==dim_loc['location'])&\\\n",
    "    (dim_atm_s1['atm_streetname']==dim_loc['street_name'])&\\\n",
    "    (dim_atm_s1['atm_street_number']==dim_loc['street_number'])&\\\n",
    "    (dim_atm_s1['atm_zipcode']==dim_loc['zipcode'])&\\\n",
    "    (dim_atm_s1['atm_lat']==dim_loc['lat'])&\\\n",
    "    (dim_atm_s1['atm_lon']==dim_loc['lon']),\n",
    "    'left'\n",
    ")\n",
    "del dim_atm_s1\n",
    "# dim_atm_s2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6147ed",
   "metadata": {},
   "source": [
    "1. The columns are rearranged and stored in *dim_atm*\n",
    "\n",
    "\n",
    "5. Column *location_id* is renamed as *atm_location_id*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "34c73da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_atm = dim_atm_s2.select(\n",
    "    'atm_id',\n",
    "    'atm_number',\n",
    "    'atm_manufacturer',\n",
    "    col('location_id').alias('atm_location_id')\n",
    ")\n",
    "del dim_atm_s2\n",
    "# print(dim_atm.count())\n",
    "# dim_atm.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77afccb",
   "metadata": {},
   "source": [
    "### **DIM_DATE**\n",
    "\n",
    "|Columns|Data Type|Constriant|\n",
    "| :- | :- |:-|\n",
    "|date_id|int|**Primary**|\n",
    "|full_date_time|string||\n",
    "|year|int||\n",
    "|month|string||\n",
    "|day|int||\n",
    "|hour|int||\n",
    "|weekday|string||"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a51929",
   "metadata": {},
   "source": [
    "1. The required library for creating the column called *full_date_time* are imported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d975f610",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import format_string,udf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bebda0",
   "metadata": {},
   "source": [
    "1. Adding a user defined function to return an integer by using a decorator.\n",
    "2. *monthToNumber* will take input as string and return the month number.\n",
    "3. The dictionary has month string as the key and value as month number\n",
    "\n",
    "\n",
    "18. The string is capitalized and used as the key to return the value from the dictionary.\n",
    "\n",
    "\n",
    "20. If the key does not exist in the dictionary 0 is returned implying that the month name is invalid.\n",
    "21. The required columns are selected for creating the date dimension and and distinct is for removing duplicates.\n",
    "22. *full_date_time* is created for stroing the time stamp generated from line 23\n",
    "23. *to_timestamp* converts the string to timestamp for that to happen the srting should be in this format \"YYYY-MM-DD hh:mm:ss\".\n",
    "24. *format_string* will create a string from values of other columns or UDF function and return the string in a format appropriate for timestamp conversion.\n",
    "25. The string or style is passed reformat the string.\n",
    "\n",
    "\n",
    "27. The UDF function *monthToNumber* is used where the column containing month string is passed\n",
    "\n",
    "\n",
    "33. A new column *date_id* is created for storing IDs\n",
    "34. The ID values are generated such that the ID starts with number 5\n",
    "35. The columns are rearranged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "58803e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date_id: integer (nullable = false)\n",
      " |-- full_date_time: string (nullable = false)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- weekday: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@udf(returnType=IntegerType())\n",
    "def monthToNumber(sMonth:str):\n",
    "    dictMonth = {\n",
    "        'January':1,\n",
    "        'February':2,\n",
    "        'March':3,\n",
    "        'April':4,\n",
    "        'May':5,\n",
    "        'June':6,\n",
    "        'July':7,\n",
    "        'August':8,\n",
    "        'September':9,\n",
    "        'October':10,\n",
    "        'November':11,\n",
    "        'December':12\n",
    "    }\n",
    "    try:\n",
    "        return dictMonth[sMonth.capitalize()]\n",
    "    except KeyError:\n",
    "        return 0\n",
    "dim_date = temp_atm.select('year','month','day','hour','weekday').distinct().withColumn(\n",
    "    'full_date_time',\n",
    "    format_string(\n",
    "        '%d-%02d-%02d %02d:00:00',\n",
    "        'year',\n",
    "        monthToNumber('month'),\n",
    "        'day',\n",
    "        'hour'\n",
    "    )\n",
    ").withColumn(\n",
    "    'date_id',\n",
    "    row_number().over(Window().orderBy('full_date_time'))*lit(2)+lit(50001)\n",
    ").select(\n",
    "    'date_id',\n",
    "    'full_date_time',\n",
    "    'year',\n",
    "    'month',\n",
    "    'day',\n",
    "    'hour',\n",
    "    'weekday'\n",
    ")\n",
    "dim_date.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e534fff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- location_id: integer (nullable = false)\n",
      " |-- location: string (nullable = true)\n",
      " |-- street_name: string (nullable = true)\n",
      " |-- street_number: integer (nullable = true)\n",
      " |-- zipcode: integer (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- lon: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_loc.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ae35df",
   "metadata": {},
   "source": [
    "##### Stage 1\n",
    "1. The dataframe *fact_atm_status_s1* will store the the join of *temp_atm* and *dim_loc* with their respective columns.\n",
    "\n",
    "\n",
    "10. The unnecessary columns are droped beforing storing in *fact_atm_status_s1*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0dcc68c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_atm_status_s1 = temp_atm.join(\n",
    "    dim_loc,\n",
    "    (temp_atm['atm_location']==dim_loc['location'])&\\\n",
    "    (temp_atm['atm_streetname']==dim_loc['street_name'])&\\\n",
    "    (temp_atm['atm_street_number']==dim_loc['street_number'])&\\\n",
    "    (temp_atm['atm_zipcode']==dim_loc['zipcode'])&\\\n",
    "    (temp_atm['atm_lat']==dim_loc['lat'])&\\\n",
    "    (temp_atm['atm_lon']==dim_loc['lon']),\n",
    "    'left'\n",
    ").drop(\n",
    "    'atm_location',\n",
    "    'location',\n",
    "    'atm_streetname',\n",
    "    'street_name',\n",
    "    'atm_street_number',\n",
    "    'street_number',\n",
    "    'atm_zipcode',\n",
    "    'zipcode',\n",
    "    'atm_lat',\n",
    "    'lat',\n",
    "    'atm_lon',\n",
    "    'lon'\n",
    ")\n",
    "# fact_atm_status_s1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f74f3a",
   "metadata": {},
   "source": [
    "##### Stage 2\n",
    "1. The dataframe *fact_atm_status_s2* will store the the join of *fact_atm_status_s1* and *dim_card_type* with their respective columns.\n",
    "\n",
    "\n",
    "5. The unnecessary columns are droped beforing storing in *fact_atm_status_s2*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e8a9b568",
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_atm_status_s2 = fact_atm_status_s1.join(\n",
    "    dim_card_type,\n",
    "    (fact_atm_status_s1['card_type']==dim_card_type['card_type']),\n",
    "    'left'\n",
    ").drop('card_type')\n",
    "# fact_atm_status_s2.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd51bac2",
   "metadata": {},
   "source": [
    "##### Stage 3\n",
    "1. The column *atm_id* is renamed to *atm_number*.<br>The column *atm_id* has the same name as the column in *dim_atm* which are different attributes. The join operation will create an extra column called *atm_id*. This will cause an exception when selecting the columns.\n",
    "2. The dataframe *fact_atm_status_s3* will store the the join of *fact_atm_status_s2* and *dim_atm* with their respective columns.\n",
    "\n",
    "\n",
    "6. The unnecessary columns are droped beforing storing in *fact_atm_status_s3*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7977a541",
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_atm_status_s2 = fact_atm_status_s2.withColumnRenamed('atm_id','atm_number')\n",
    "fact_atm_status_s3 = fact_atm_status_s2.join(\n",
    "    dim_atm,\n",
    "    (fact_atm_status_s2['atm_number']==dim_atm['atm_number'])&\\\n",
    "    (fact_atm_status_s2['atm_manufacturer']==dim_atm['atm_manufacturer'])\n",
    ").drop('atm_number','atm_manufacturer','atm_location_id')\n",
    "# fact_atm_status_s3.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d9fdd0",
   "metadata": {},
   "source": [
    "##### Stage 4\n",
    "1. The dataframe *fact_atm_status_s4* will store the the join of *fact_atm_status_s3* and *dim_date* with their respective columns.\n",
    "\n",
    "\n",
    "5. The unnecessary columns are droped beforing storing in *fact_atm_status_s2*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "65a8bb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_atm_status_s4 = fact_atm_status_s3.join(\n",
    "    dim_date,\n",
    "    (fact_atm_status_s3['year']==dim_date['year'])&\\\n",
    "    (fact_atm_status_s3['month']==dim_date['month'])&\\\n",
    "    (fact_atm_status_s3['day']==dim_date['day'])&\\\n",
    "    (fact_atm_status_s3['hour']==dim_date['hour'])&\\\n",
    "    (fact_atm_status_s3['weekday']==dim_date['weekday'])\n",
    ").drop('year','month','day','hour','weekday','full_date_time')\n",
    "# fact_atm_status_s4.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f50cced",
   "metadata": {},
   "source": [
    "### **FACT_ATM_TRANS**\n",
    "|Columns|Data Type|Constriant|\n",
    "|:-|:-|:-|\n",
    "|tran_id|bigint|**Primary**|\n",
    "|atm_id|int|**Foreign(dim_atm.atm_id)**|\n",
    "|weather_loc_id|int|**Foreign(dim_location.location_id)**|\n",
    "|date_id|int|**Foreign(dim_date.date_id)**|\n",
    "|card_type_id|int|**Foreign(dim_card_type.card_type_id)**|\n",
    "|atm_status|string|\n",
    "|curency|string|\n",
    "|service|string|\n",
    "|transaction_amount|int|\n",
    "|message_code|string|\n",
    "|message_text|string|\n",
    "|rain_3h|float|\n",
    "|cloud_all|int|\n",
    "|weather_id|int|\n",
    "|weather_main|string|\n",
    "|weather_description|string|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940efc68",
   "metadata": {},
   "source": [
    "##### Transaction ID\n",
    "2. A new column *tran_id* is created for storing transaction IDs.\n",
    "3. It returns the row number each line of the dataframe which is added with a long number\n",
    "\n",
    "The large number is for identifying the ID as transaction ID.<br>Since the tran_id data type is *bigint* a suitable number is added to match the size of the data type\n",
    "\n",
    "<font color='red'>\\* The numbered list are comments for the code bellow</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6e29690f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_atm_status_s4 = fact_atm_status_s4.withColumn(\n",
    "    'tran_id',\n",
    "    row_number().over(Window().orderBy(lit('a')))+lit(2017000000000001)\n",
    "    \n",
    ")\n",
    "# temp_atm.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23106c2",
   "metadata": {},
   "source": [
    "1. The required columns are selected and renamed according to the above Fact table schema and stored in *fact_atm_tran*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "582fcb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_atm_tran = fact_atm_status_s4.select(\n",
    "    'tran_id',\n",
    "    'atm_id',\n",
    "    col('location_id').alias('weather_loc_id'),\n",
    "    'date_id',\n",
    "    'card_type_id',\n",
    "    'atm_status',\n",
    "    'currency',\n",
    "    'service',\n",
    "    'transaction_amount',\n",
    "    'message_code',\n",
    "    'message_text',\n",
    "    'rain_3h',\n",
    "    'clouds_all',\n",
    "    'weather_id',\n",
    "    'weather_main',\n",
    "    'weather_description'\n",
    ").coalesce(1)\n",
    "# fact_atm_tran.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90575337",
   "metadata": {},
   "source": [
    "2. The null values in *fact_atm_tran* columns *message_code* and *message_text* is filled.\n",
    "3. The double quotes in the string was causing error when being imported in to redshift. I have removed them using *regexp_replace* function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2b30eeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace\n",
    "fact_atm_tran = fact_atm_tran.fillna('0000',subset=['message_code']).fillna('Success',subset=['message_text'])\n",
    "# fact_atm_tran.select('message_code','message_text').distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74418ccf",
   "metadata": {},
   "source": [
    "##### S3 Path where csv files are created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9edc0e16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d74d07ba2cf45c381596d63857d5534",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "s3Path = 's3://sharads-etl-project/Data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6040fbac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f698e32b2c1047218db6bdf8b0c63cdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fact_atm_tran.write.csv(s3Path+'fact_atm_tran1')\n",
    "\n",
    "dim_loc.write.csv(s3Path+'dim_loc')\n",
    "\n",
    "dim_atm.write.csv(s3Path+'dim_atm')\n",
    "\n",
    "dim_date.write.csv(s3Path+'dim_date')\n",
    "\n",
    "dim_card_type.write.csv(s3Path+'dim_card_type')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
